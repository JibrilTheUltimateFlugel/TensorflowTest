{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientTapeMNISTTutor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPS/jzwofCBeNjuvzlfhcba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JibrilTheUltimateFlugel/TensorflowTest/blob/main/GradientTapeMNISTTutor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6KwcRGaEMyW",
        "outputId": "d3d3d900-3ee7-47fb-e74d-385ddd8275b9"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "\n",
        "def build_model(width, height, depth, classes):\n",
        "\t# initialize the input shape and channels dimension to be\n",
        "\t# \"channels last\" ordering\n",
        "\tinputShape = (height, width, depth)\n",
        "\tchanDim = -1\n",
        "\t# build the model using Keras' Sequential API\n",
        "\tmodel = Sequential([\n",
        "\t\t# CONV => RELU => BN => POOL layer set\n",
        "\t\tConv2D(16, (3, 3), padding=\"same\", input_shape=inputShape),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
        "\t\t# (CONV => RELU => BN) * 2 => POOL layer set\n",
        "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
        "\t\t# (CONV => RELU => BN) * 3 => POOL layer set\n",
        "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
        "\t\t# first (and only) set of FC => RELU layers\n",
        "\t\tFlatten(),\n",
        "\t\tDense(256),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(),\n",
        "\t\tDropout(0.5),\n",
        "\t\t# softmax classifier\n",
        "\t\tDense(classes),\n",
        "\t\tActivation(\"softmax\")\n",
        "\t])\n",
        "\t# return the built model to the calling function\n",
        "\treturn model\n",
        "\n",
        "def step(X, y):\n",
        "\t# keep track of our gradients\n",
        "\twith tf.GradientTape() as tape:\n",
        "\t\t# make a prediction using the model and then calculate the\n",
        "\t\t# loss\n",
        "\t\tpred = model(X)\n",
        "\t\tloss = categorical_crossentropy(y, pred)\n",
        "\t# calculate the gradients using our tape and then update the\n",
        "\t# model weights\n",
        "\tgrads = tape.gradient(loss, model.trainable_variables)\n",
        "\topt.apply_gradients(zip(grads, model.trainable_variables))\n",
        " \n",
        "# initialize the number of epochs to train for, batch size, and\n",
        "# initial learning rate\n",
        "EPOCHS = 25\n",
        "BS = 64\n",
        "INIT_LR = 1e-3\n",
        "# load the MNIST dataset\n",
        "print(\"[INFO] loading MNIST dataset...\")\n",
        "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
        "# add a channel dimension to every image in the dataset, then scale\n",
        "# the pixel intensities to the range [0, 1]\n",
        "trainX = np.expand_dims(trainX, axis=-1)\n",
        "testX = np.expand_dims(testX, axis=-1)\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "# one-hot encode the labels\n",
        "trainY = to_categorical(trainY, 10)\n",
        "testY = to_categorical(testY, 10)\n",
        "\n",
        "# build our model and initialize our optimizer\n",
        "print(\"[INFO] creating model...\")\n",
        "model = build_model(28, 28, 1, 10)\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "\n",
        "# compute the number of batch updates per epoch\n",
        "numUpdates = int(trainX.shape[0] / BS)\n",
        "# loop over the number of epochs\n",
        "for epoch in range(0, EPOCHS):\n",
        "\t# show the current epoch number\n",
        "\tprint(\"[INFO] starting epoch {}/{}...\".format(\n",
        "\t\tepoch + 1, EPOCHS), end=\"\")\n",
        "\tsys.stdout.flush()\n",
        "\tepochStart = time.time()\n",
        "\t# loop over the data in batch size increments\n",
        "\tfor i in range(0, numUpdates):\n",
        "\t\t# determine starting and ending slice indexes for the current\n",
        "\t\t# batch\n",
        "\t\tstart = i * BS\n",
        "\t\tend = start + BS\n",
        "\t\t# take a step\n",
        "\t\tstep(trainX[start:end], trainY[start:end])\n",
        "\t# show timing information for the epoch\n",
        "\tepochEnd = time.time()\n",
        "\telapsed = (epochEnd - epochStart) / 60.0\n",
        "\tprint(\"took {:.4} minutes\".format(elapsed))\n",
        " \n",
        " # in order to calculate accuracy using Keras' functions we first need\n",
        "# to compile the model\n",
        "model.compile(optimizer=opt, loss=categorical_crossentropy,\n",
        "\tmetrics=[\"acc\"])\n",
        "# now that the model is compiled we can compute the accuracy\n",
        "(loss, acc) = model.evaluate(testX, testY)\n",
        "print(\"[INFO] test accuracy: {:.4f}\".format(acc))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading MNIST dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "[INFO] creating model...\n",
            "[INFO] starting epoch 1/25..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "took 2.172 minutes\n",
            "[INFO] starting epoch 2/25...took 2.149 minutes\n",
            "[INFO] starting epoch 3/25...took 2.188 minutes\n",
            "[INFO] starting epoch 4/25...took 2.163 minutes\n",
            "[INFO] starting epoch 5/25...took 2.168 minutes\n",
            "[INFO] starting epoch 6/25...took 2.153 minutes\n",
            "[INFO] starting epoch 7/25...took 2.203 minutes\n",
            "[INFO] starting epoch 8/25...took 2.145 minutes\n",
            "[INFO] starting epoch 9/25...took 2.137 minutes\n",
            "[INFO] starting epoch 10/25...took 2.144 minutes\n",
            "[INFO] starting epoch 11/25...took 2.144 minutes\n",
            "[INFO] starting epoch 12/25...took 2.159 minutes\n",
            "[INFO] starting epoch 13/25...took 2.141 minutes\n",
            "[INFO] starting epoch 14/25...took 2.178 minutes\n",
            "[INFO] starting epoch 15/25...took 2.124 minutes\n",
            "[INFO] starting epoch 16/25...took 2.177 minutes\n",
            "[INFO] starting epoch 17/25...took 2.124 minutes\n",
            "[INFO] starting epoch 18/25...took 2.136 minutes\n",
            "[INFO] starting epoch 19/25...took 2.138 minutes\n",
            "[INFO] starting epoch 20/25...took 2.145 minutes\n",
            "[INFO] starting epoch 21/25...took 2.15 minutes\n",
            "[INFO] starting epoch 22/25...took 2.164 minutes\n",
            "[INFO] starting epoch 23/25...took 2.14 minutes\n",
            "[INFO] starting epoch 24/25...took 2.131 minutes\n",
            "[INFO] starting epoch 25/25...took 2.13 minutes\n",
            "313/313 [==============================] - 6s 17ms/step - loss: 0.0320 - acc: 0.9936\n",
            "[INFO] test accuracy: 0.9936\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}