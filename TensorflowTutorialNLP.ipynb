{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorflowTutorialNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWIC4SSQOCvqKFG1VXv0o5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JibrilTheUltimateFlugel/TensorflowTest/blob/main/TensorflowTutorialNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vh9Qm1A2h-z",
        "outputId": "2fa239b8-cb0c-435f-8543-97d46e5fd10b"
      },
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
        "  bag = {}  # stores all of the encodings and their frequency\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      encoding = vocab[word]  # get encoding from vocab\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding = word_encoding\n",
        "      word_encoding += 1\n",
        "    \n",
        "    if encoding in bag:\n",
        "      bag[encoding] += 1\n",
        "    else:\n",
        "      bag[encoding] = 1\n",
        "  \n",
        "  return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4sFwWvyNP3S",
        "outputId": "78bb8833-847c-4424-d56a-b1e12fc4c725"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooGEAbZENokI",
        "outputId": "d1ffaffc-7d4c-4ac5-e0ea-f451c6eccff6"
      },
      "source": [
        "# Lets look at one review\n",
        "train_data[1]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 194,\n",
              " 1153,\n",
              " 194,\n",
              " 8255,\n",
              " 78,\n",
              " 228,\n",
              " 5,\n",
              " 6,\n",
              " 1463,\n",
              " 4369,\n",
              " 5012,\n",
              " 134,\n",
              " 26,\n",
              " 4,\n",
              " 715,\n",
              " 8,\n",
              " 118,\n",
              " 1634,\n",
              " 14,\n",
              " 394,\n",
              " 20,\n",
              " 13,\n",
              " 119,\n",
              " 954,\n",
              " 189,\n",
              " 102,\n",
              " 5,\n",
              " 207,\n",
              " 110,\n",
              " 3103,\n",
              " 21,\n",
              " 14,\n",
              " 69,\n",
              " 188,\n",
              " 8,\n",
              " 30,\n",
              " 23,\n",
              " 7,\n",
              " 4,\n",
              " 249,\n",
              " 126,\n",
              " 93,\n",
              " 4,\n",
              " 114,\n",
              " 9,\n",
              " 2300,\n",
              " 1523,\n",
              " 5,\n",
              " 647,\n",
              " 4,\n",
              " 116,\n",
              " 9,\n",
              " 35,\n",
              " 8163,\n",
              " 4,\n",
              " 229,\n",
              " 9,\n",
              " 340,\n",
              " 1322,\n",
              " 4,\n",
              " 118,\n",
              " 9,\n",
              " 4,\n",
              " 130,\n",
              " 4901,\n",
              " 19,\n",
              " 4,\n",
              " 1002,\n",
              " 5,\n",
              " 89,\n",
              " 29,\n",
              " 952,\n",
              " 46,\n",
              " 37,\n",
              " 4,\n",
              " 455,\n",
              " 9,\n",
              " 45,\n",
              " 43,\n",
              " 38,\n",
              " 1543,\n",
              " 1905,\n",
              " 398,\n",
              " 4,\n",
              " 1649,\n",
              " 26,\n",
              " 6853,\n",
              " 5,\n",
              " 163,\n",
              " 11,\n",
              " 3215,\n",
              " 10156,\n",
              " 4,\n",
              " 1153,\n",
              " 9,\n",
              " 194,\n",
              " 775,\n",
              " 7,\n",
              " 8255,\n",
              " 11596,\n",
              " 349,\n",
              " 2637,\n",
              " 148,\n",
              " 605,\n",
              " 15358,\n",
              " 8003,\n",
              " 15,\n",
              " 123,\n",
              " 125,\n",
              " 68,\n",
              " 23141,\n",
              " 6853,\n",
              " 15,\n",
              " 349,\n",
              " 165,\n",
              " 4362,\n",
              " 98,\n",
              " 5,\n",
              " 4,\n",
              " 228,\n",
              " 9,\n",
              " 43,\n",
              " 36893,\n",
              " 1157,\n",
              " 15,\n",
              " 299,\n",
              " 120,\n",
              " 5,\n",
              " 120,\n",
              " 174,\n",
              " 11,\n",
              " 220,\n",
              " 175,\n",
              " 136,\n",
              " 50,\n",
              " 9,\n",
              " 4373,\n",
              " 228,\n",
              " 8255,\n",
              " 5,\n",
              " 25249,\n",
              " 656,\n",
              " 245,\n",
              " 2350,\n",
              " 5,\n",
              " 4,\n",
              " 9837,\n",
              " 131,\n",
              " 152,\n",
              " 491,\n",
              " 18,\n",
              " 46151,\n",
              " 32,\n",
              " 7464,\n",
              " 1212,\n",
              " 14,\n",
              " 9,\n",
              " 6,\n",
              " 371,\n",
              " 78,\n",
              " 22,\n",
              " 625,\n",
              " 64,\n",
              " 1382,\n",
              " 9,\n",
              " 8,\n",
              " 168,\n",
              " 145,\n",
              " 23,\n",
              " 4,\n",
              " 1690,\n",
              " 15,\n",
              " 16,\n",
              " 4,\n",
              " 1355,\n",
              " 5,\n",
              " 28,\n",
              " 6,\n",
              " 52,\n",
              " 154,\n",
              " 462,\n",
              " 33,\n",
              " 89,\n",
              " 78,\n",
              " 285,\n",
              " 16,\n",
              " 145,\n",
              " 95]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I9OPYplQblR"
      },
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ5tsvpJQ6PR"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KScmU7yEQ7PI",
        "outputId": "33af2bff-969f-4fe7-d7a2-d853d9b7c703"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 32)          2834688   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_U8HaRcRMff",
        "outputId": "c867810e-7894-46ac-b657-6928616af03e"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 69s 62ms/step - loss: 0.5262 - acc: 0.7242 - val_loss: 0.2806 - val_acc: 0.8876\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.2457 - acc: 0.9079 - val_loss: 0.2652 - val_acc: 0.8950\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.1799 - acc: 0.9352 - val_loss: 0.2725 - val_acc: 0.9002\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.1480 - acc: 0.9493 - val_loss: 0.2764 - val_acc: 0.8944\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.1261 - acc: 0.9576 - val_loss: 0.3033 - val_acc: 0.8914\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.1089 - acc: 0.9632 - val_loss: 0.3352 - val_acc: 0.8884\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.0892 - acc: 0.9702 - val_loss: 0.3215 - val_acc: 0.8862\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.0806 - acc: 0.9739 - val_loss: 0.3184 - val_acc: 0.8848\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.0718 - acc: 0.9761 - val_loss: 0.3822 - val_acc: 0.8696\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.0681 - acc: 0.9796 - val_loss: 0.3667 - val_acc: 0.8818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsRSaVzDRmdh",
        "outputId": "11292f6d-34dc-4a18-f407-93b9aa36aae7"
      },
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4297 - acc: 0.8568\n",
            "[0.42969444394111633, 0.8568000197410583]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f90ZJjaTgK2E",
        "outputId": "52f42519-c06f-4b2b-f4a5-38612f6a5e41"
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWTG7h3rhDLj",
        "outputId": "d70ce1ec-d943-459c-87c3-b7a49d9ad5d6"
      },
      "source": [
        "# while were at it lets make a decode function\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]\n",
        "  \n",
        "print(decode_integers(encoded))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that movie was just amazing so amazing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyxsoVWGhHbx",
        "outputId": "d0610674-d90c-45e2-b9e3-80067e2c43c9"
      },
      "source": [
        "# now time to make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.89587575]\n",
            "[0.28983203]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYvgk1lriNGU",
        "outputId": "44fc0f08-a026-45c0-cf9d-a910f04bfd43"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ug9rgediWz0",
        "outputId": "dbeb2604-8483-409a-e613-349c13a90652"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAfy7vVji1QL",
        "outputId": "459befe6-c85c-4a0c-abd3-3e34f3668406"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4whiSuhi2Ob",
        "outputId": "ba82d688-647c-4c21-ecaf-ac319430b581"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl17rQx2kbTM"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjOlMNbxkgXs",
        "outputId": "e32f5e5d-1426-4074-bc44-22484cbbfa7a"
      },
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCjDTJIGkqSr",
        "outputId": "3ee67d49-743f-492b-9f0e-7977726c343c"
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DpJpbmylS1-"
      },
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME7isSbJmNrs"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqv7WSnRmPI7"
      },
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u46H4HCzmzJT",
        "outputId": "bdb4f93e-60cb-42c0-ce05-d70f42340e41"
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crLIIzs3nG83"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAS_DVIdn7em",
        "outputId": "6ea0c125-994e-4ae1-8dca-e3dc4996a732"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1Vwxd8_o181",
        "outputId": "c52c160c-70a7-4fc5-f4c9-4999daf44377"
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxDvz3H9pUvl",
        "outputId": "82bbdfa2-70f1-4b9f-fba9-b932cf0bebf0"
      },
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 1.66827044e-03  1.55216677e-03 -2.17856327e-03 ... -3.14404350e-03\n",
            "    2.18022708e-03 -2.39616693e-05]\n",
            "  [ 3.51096457e-03 -1.64700861e-04  5.73062105e-03 ...  5.24737479e-05\n",
            "    6.81704050e-03  2.17994954e-03]\n",
            "  [ 5.88883739e-03  1.64491881e-03  8.02147295e-03 ...  6.90988614e-04\n",
            "    7.34932907e-03  2.50946637e-03]\n",
            "  ...\n",
            "  [ 8.60568788e-03 -3.89562361e-03  1.34243546e-02 ... -3.23499134e-03\n",
            "   -4.20431979e-03  8.39823019e-03]\n",
            "  [ 8.14974122e-03 -1.25051849e-03  7.25988485e-03 ... -4.41040518e-03\n",
            "   -3.64979776e-03  5.95726585e-03]\n",
            "  [ 9.28679015e-03  2.16523325e-03  8.27038754e-03 ... -1.49674551e-03\n",
            "   -2.25334009e-03  5.07457927e-03]]\n",
            "\n",
            " [[ 3.75708169e-03 -1.75408169e-03  2.52010883e-03 ... -5.59973065e-03\n",
            "    3.44440527e-03  1.85888272e-03]\n",
            "  [ 5.15085598e-03 -4.14640550e-03  4.95234411e-03 ... -5.15715079e-03\n",
            "   -3.90450004e-03  3.99021385e-03]\n",
            "  [ 5.51201124e-03 -1.11230195e-03  3.47935042e-04 ... -6.21052226e-03\n",
            "   -2.42231530e-03  2.97582266e-03]\n",
            "  ...\n",
            "  [-1.86895882e-03  3.90134403e-03  7.05427228e-05 ...  1.52496935e-03\n",
            "   -1.00082010e-02 -1.50663108e-02]\n",
            "  [ 2.01292383e-03  6.59934198e-03  3.85551224e-03 ...  2.71642930e-03\n",
            "   -8.11587833e-03 -9.51009616e-03]\n",
            "  [ 2.27017037e-04  2.39299121e-03  6.48336485e-03 ...  5.05558308e-03\n",
            "   -5.26029011e-03 -6.48269430e-03]]\n",
            "\n",
            " [[ 3.03159165e-03 -3.46902199e-03  3.74113000e-03 ... -9.98534146e-04\n",
            "   -5.36287995e-03  1.82537676e-03]\n",
            "  [ 1.64372846e-03  1.73449493e-03  3.74767842e-04 ... -1.65785674e-03\n",
            "   -7.18361465e-03  1.42347580e-03]\n",
            "  [ 5.57331927e-03  3.59857362e-03  3.92910233e-03 ... -2.27423920e-03\n",
            "   -9.54842567e-03  7.39021169e-04]\n",
            "  ...\n",
            "  [ 8.39948282e-03  1.88479945e-03  4.61103348e-03 ... -1.63744335e-04\n",
            "   -1.35375625e-02  3.47425183e-03]\n",
            "  [ 1.20746326e-02 -2.00309718e-04  2.18636007e-03 ...  5.17987553e-03\n",
            "   -1.13024805e-02 -4.53901303e-04]\n",
            "  [ 1.35999341e-02  3.09829693e-03  5.04278950e-03 ...  6.54624170e-03\n",
            "   -8.03168304e-03 -7.26662867e-04]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-4.78705112e-03 -4.93755890e-03  2.34952196e-03 ... -3.03633348e-03\n",
            "   -2.20806757e-03 -3.42607219e-03]\n",
            "  [-1.68393343e-03 -4.41828417e-03  9.44518577e-03 ...  4.00346762e-04\n",
            "    2.41439184e-03 -1.22649653e-03]\n",
            "  [ 1.77678966e-03 -9.48144880e-04  1.11058000e-02 ...  1.32082507e-03\n",
            "    3.15773673e-03 -4.31773777e-04]\n",
            "  ...\n",
            "  [-3.36825312e-03 -8.97753797e-03  3.93810216e-03 ... -6.32102136e-03\n",
            "   -2.79792730e-04 -5.55893872e-03]\n",
            "  [-2.73884297e-03 -4.04828275e-03 -3.44139733e-03 ... -4.17159358e-03\n",
            "    2.44530081e-03 -4.65545803e-03]\n",
            "  [ 2.07288470e-03 -5.53404773e-03  3.67160654e-04 ... -9.59162787e-03\n",
            "    3.87056265e-03 -7.40794349e-05]]\n",
            "\n",
            " [[ 2.58323620e-03  2.61517079e-03  3.01413820e-03 ...  1.05876010e-03\n",
            "    2.10025534e-03  1.08903181e-03]\n",
            "  [ 1.04712683e-03  2.41058902e-03  2.95806956e-03 ... -3.91584821e-03\n",
            "   -3.65690957e-03  4.80571995e-03]\n",
            "  [ 5.92879346e-03  4.13113739e-03  6.97287964e-03 ... -3.60546797e-03\n",
            "   -6.67208200e-03  3.11690336e-03]\n",
            "  ...\n",
            "  [ 9.72766802e-03 -3.09399911e-03  1.23613828e-03 ...  4.27280134e-03\n",
            "   -8.50897934e-03  3.16346908e-04]\n",
            "  [ 1.07844146e-02  7.56132533e-04  3.17606819e-03 ...  5.88795403e-03\n",
            "   -6.71576569e-03  6.95415147e-05]\n",
            "  [ 3.00699589e-03 -4.52568708e-03 -6.01160107e-04 ...  1.58588972e-03\n",
            "   -9.10350960e-03 -9.58075002e-03]]\n",
            "\n",
            " [[ 2.58323620e-03  2.61517079e-03  3.01413820e-03 ...  1.05876010e-03\n",
            "    2.10025534e-03  1.08903181e-03]\n",
            "  [-1.20903300e-04 -1.36637839e-03  5.08232089e-03 ...  3.07754450e-03\n",
            "    4.51305462e-03  1.25437987e-03]\n",
            "  [ 3.11694294e-03  1.48446916e-03  8.64953175e-03 ...  1.09690614e-03\n",
            "   -8.52242112e-04  7.73048727e-04]\n",
            "  ...\n",
            "  [-1.78494432e-03 -8.76816269e-03  5.12707978e-03 ... -6.25339756e-03\n",
            "   -3.44396895e-03 -8.88017099e-03]\n",
            "  [-8.71670840e-04 -8.76490865e-03  8.86434875e-03 ... -7.39317201e-03\n",
            "   -7.76424725e-03 -5.11499541e-03]\n",
            "  [ 1.05535546e-04 -8.29821173e-03  1.09046511e-02 ... -8.21211841e-03\n",
            "   -1.02602150e-02 -2.97540776e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzIAAjdwpXbP",
        "outputId": "3c840753-bda1-4ddc-9b83-f9a6c214d2d8"
      },
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 1.6682704e-03  1.5521668e-03 -2.1785633e-03 ... -3.1440435e-03\n",
            "   2.1802271e-03 -2.3961669e-05]\n",
            " [ 3.5109646e-03 -1.6470086e-04  5.7306211e-03 ...  5.2473748e-05\n",
            "   6.8170405e-03  2.1799495e-03]\n",
            " [ 5.8888374e-03  1.6449188e-03  8.0214730e-03 ...  6.9098861e-04\n",
            "   7.3493291e-03  2.5094664e-03]\n",
            " ...\n",
            " [ 8.6056879e-03 -3.8956236e-03  1.3424355e-02 ... -3.2349913e-03\n",
            "  -4.2043198e-03  8.3982302e-03]\n",
            " [ 8.1497412e-03 -1.2505185e-03  7.2598848e-03 ... -4.4104052e-03\n",
            "  -3.6497978e-03  5.9572659e-03]\n",
            " [ 9.2867902e-03  2.1652333e-03  8.2703875e-03 ... -1.4967455e-03\n",
            "  -2.2533401e-03  5.0745793e-03]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_n6wJB2rhGN",
        "outputId": "b939a9ab-cae2-45a0-def4-4da09e178468"
      },
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 1.6682704e-03  1.5521668e-03 -2.1785633e-03 -2.3368769e-03\n",
            "  3.5667752e-03 -3.3412850e-04  8.0102799e-04  2.0578711e-03\n",
            "  4.1105431e-03  4.6432470e-03 -8.2232308e-04  2.0946393e-04\n",
            "  2.2583038e-03  3.9400649e-03  3.5549940e-03  2.8132533e-03\n",
            " -1.7047818e-04  3.1942835e-03  4.9534659e-03 -5.4903463e-03\n",
            " -3.4515403e-04 -1.5631063e-03  3.5062726e-03  4.3356279e-03\n",
            " -1.5077464e-03  1.2654602e-03 -9.7390637e-04 -2.1757800e-03\n",
            "  3.1903572e-03 -2.4386647e-03 -3.3281003e-03 -1.9536132e-03\n",
            "  1.3816865e-03 -1.7728631e-03  3.8139641e-04  6.0047442e-03\n",
            "  5.4533640e-03  4.4890816e-04  5.9374310e-03 -2.6946471e-03\n",
            "  2.4641142e-03  1.8618285e-03  5.0086423e-04 -4.5922599e-03\n",
            "  1.7566068e-04 -2.6001660e-03  2.3825706e-03  5.6884559e-03\n",
            " -2.9879131e-03 -2.5320104e-03  7.1012502e-04  1.5043099e-03\n",
            " -5.9142513e-03 -1.7696500e-03  1.7350102e-03  2.2123619e-03\n",
            " -2.4750601e-03  4.2750239e-03 -3.8939866e-03 -3.0016684e-04\n",
            "  4.5350485e-04 -4.8900056e-03 -3.1440435e-03  2.1802271e-03\n",
            " -2.3961669e-05], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W7a1FzFGrlHo",
        "outputId": "f8d2c33b-f192-43c7-ec8b-0c0ced34dacc"
      },
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"A$$n.Ynttu\\njmPTlrdO\\nAovqK\\nZXTnnZYLUM ZO\\nG-BxejAedYNaaCS;W3A$yqjvdhF3Nf:.'AMLZNWO$Qx.3Fhj!ftoWWUf3DHt\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APjxzqffsdpw"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSfVXG09sfK3"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mKtBD-hsgUn"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV5QVRjnshha",
        "outputId": "9ca888ec-6ce2-462b-a3bc-5c33db5460c4"
      },
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 31s 160ms/step - loss: 3.0838\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 29s 161ms/step - loss: 1.9969\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.7046\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.5457\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 29s 163ms/step - loss: 1.4498\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 29s 163ms/step - loss: 1.3825\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.3329\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.2958\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.2600\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.2203\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.1861\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.1518\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.1157\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.0774\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.0377\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.9973\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.9564\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.9143\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.8732\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.8351\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.7989\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.7628\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.7283\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.6992\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.6693\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.6425\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.6191\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 0.5976\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 29s 163ms/step - loss: 0.5797\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 29s 163ms/step - loss: 0.5611\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.5441\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 29s 163ms/step - loss: 0.5294\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.5166\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.5032\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4965\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4847\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4774\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4695\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4616\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4550\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4511\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4448\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4387\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4338\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4303\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4261\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4254\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4189\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4166\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVoOOchyzog2"
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWcl41flzpVg"
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "DbZ6cXtSzuVn",
        "outputId": "953aea3e-f2b5-4a3f-9dfa-a1225df82be3"
      },
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-97614ead033d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./training_checkpoints/ckpt_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m       \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_is_hdf5_filepath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2814\u001b[0;31m   return (filepath.endswith('.h5') or filepath.endswith('.keras') or\n\u001b[0m\u001b[1;32m   2815\u001b[0m           filepath.endswith('.hdf5'))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python._pywrap_checkpoint_reader.Checkp' object has no attribute 'endswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEIL68iR1D4v"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1zmjbZF1Gli",
        "outputId": "2c9a3e00-6107-474d-db54-11263a785340"
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: capulet\n",
            "capulets forfeir of the side\n",
            "Silver a word, which at home is mackion.\n",
            "\n",
            "ESCALUS:\n",
            "Why, that my daughter? where is my wound to hear?\n",
            "\n",
            "KATHARINA:\n",
            "Milloppiness attended home,\n",
            "And I am ready stary riches flown;\n",
            "As to seem the bottom of my grief,\n",
            "Toou plot\n",
            "the insinuating nod and Citizens:\n",
            "All things that from his speed\n",
            "Through lamentably commended\n",
            "him; his unstuff'd out a carfful page?\n",
            "Besides, I have pardon\n",
            "With cozen wips of my certainly acrown,\n",
            "Stands keep the maid: and, like a undertaking.\n",
            "\n",
            "Third Citizen:\n",
            "But there was son.\n",
            "\n",
            "LADY ANNE:\n",
            "What down this was thick, that holp to joy,\n",
            "And they high fortune eng;\n",
            "The correct be the worst for brand: say that Friar John Bolingbroke.\n",
            "\n",
            "KING RICHARD II:\n",
            "Why, that's the friar of boldness live, upon\n",
            "He will ask thou had run swift, he should be er,\n",
            "I pritten with \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}